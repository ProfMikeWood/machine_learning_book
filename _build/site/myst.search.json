{"version":"1","records":[{"hierarchy":{"lvl1":"Linear Regression as a Machine Learning Model"},"type":"lvl1","url":"/linear-regression","position":0},{"hierarchy":{"lvl1":"Linear Regression as a Machine Learning Model"},"content":"In this notebook, we will consider linear regression and how it might be approached as a machine learning model.\n\nLearning Objectives\n\nAt the end of this notebook, you should be able to\n\nenvision linear regression as an iterative process.\n\ndefine the mean square error cost function and compute its gradient\n\ndescribe how gradient descent is used to final optimal model parameters.\n\nImport modules\n\nBegin by importing the modules to be used in this notebook\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\n\n\n\n","type":"content","url":"/linear-regression","position":1},{"hierarchy":{"lvl1":"Linear Regression as a Machine Learning Model","lvl2":"Linear Regression"},"type":"lvl2","url":"/linear-regression#linear-regression","position":2},{"hierarchy":{"lvl1":"Linear Regression as a Machine Learning Model","lvl2":"Linear Regression"},"content":"In its simplest form, linear regression refers to fitting a simple line to data. In our early math classes, we learn that a line has the following form:y = mx+b\n\nwhere x is some independent data, y is some dependent data, m is the slope and b is the intercept.\n\nAs introduced in the previous notebook, we’ll use the monthly_in_situ_co2_mlo.csv file provided in the data direcotry. Let’s read that in again:\n\n# read in the dataset with pandas\ndf = pd.read_csv('../data/monthly_in_situ_co2_mlo.csv', skiprows=64)\n\n# filter out null values stored as -99\ndf = df[df.iloc[:, 4] >0]\n\n# the decimal year information is in the 4th column\nx = df.iloc[:, 3]\n\n# the CO2 information is in the 5th column\ny = df.iloc[:, 4]\n\n\n\nFor the purposes of this demo, let’s subset our data to be during the period 2010-2020 (don’t worry, we’ll come back to the full dataset in a subsequent next notebook).\n\n# subset to a given time range\ndf_subset = df[(x>=2010) & (x<=2020)]\n\n# redefine x and y\nx = np.array(df_subset.iloc[:,3])\ny = np.array(df_subset.iloc[:,4])\n\n# remove the first value of y\nx = x-2010\ny = y-y[0]\n\n\n\nplt.figure(figsize=(8,4))\nplt.plot(x,y,'k.')\nplt.xlabel('x (years since 2010)')\nplt.ylabel('y (CO$_2$ concentration relative to 2010, ppm)')\nplt.show()\n\n\n\nLooking at the data, we can see that the curve has a faily consistent upward trend amid a seasonal trend. One question we may be interested in is: how much has the CO_2 concentration been increasing each year?","type":"content","url":"/linear-regression#linear-regression","position":3},{"hierarchy":{"lvl1":"Linear Regression as a Machine Learning Model","lvl2":"Linear Regression with numpy"},"type":"lvl2","url":"/linear-regression#linear-regression-with-numpy","position":4},{"hierarchy":{"lvl1":"Linear Regression as a Machine Learning Model","lvl2":"Linear Regression with numpy"},"content":"We can fit a polynomial to this data using numpy as follows to find the slope of the line of best fit:\n\np = np.polyfit(x,y,deg=1)\nm = p[0]\nb = p[1]\nprint('Intercept: ',b)\nprint('Slope: ',m)\n\n\n\nThis slope defines the rate of increase each year - here we estimate that on average during 2010-2020, the rate of increase was about 2.37 ppm/year.\n\nWe can plot the model on an independent x-axis as follows:\n\n# define a model mx+b and apply it over the range of data values\nmodel_x = np.arange(np.min(x),np.max(x),0.1)\nmodel_y = m*model_x+b\n\n# recreate the plot with the line of best fit\nplt.figure(figsize=(8,4))\nplt.plot(x,y,'k.',label='data')\nplt.plot(model_x, model_y,label='model')\nplt.xlabel('x (year)')\nplt.ylabel('y (CO$_2$ concentration, ppm)')\nplt.legend(loc=2)\nplt.show()\n\n\n\n","type":"content","url":"/linear-regression#linear-regression-with-numpy","position":5},{"hierarchy":{"lvl1":"Linear Regression as a Machine Learning Model","lvl3":"The mechanics of linear regression","lvl2":"Linear Regression with numpy"},"type":"lvl3","url":"/linear-regression#the-mechanics-of-linear-regression","position":6},{"hierarchy":{"lvl1":"Linear Regression as a Machine Learning Model","lvl3":"The mechanics of linear regression","lvl2":"Linear Regression with numpy"},"content":"Neat - we can pretty easily fit a line to some data. But what happened under the hood?\n\nIn this process, numpy has minimized the error between the points and the line to find the best estimates of the slope m and the slope b. In most math textbooks, the error between the data and model is referred to as a “cost function” and, in this case, is given asJ = \\frac{1}{N} \\sum_{i=1}^N (y_{data}(i) - y_{model}(i))^2\n\nThis formula is called the Mean Squared Error (MSE) since, well, it is the mean of the errors squared.\n\nWe can code this up as function using numpy as:\n\ndef cost_function(y_data, y_modeled):\n    N = np.size(y_data)\n    mean_squared_error = (1/N)*np.sum((y_data-y_modeled)**2)\n    return(mean_squared_error)\n\n\n\nUsing the above cost function, we can compute and visualize the “error space” - the cost between data and model for a range of the parameters m and b.\n\nintercept_space = np.linspace(-20,20,200)\nslope_space = np.linspace(0,5,200)\nI, S = np.meshgrid(intercept_space, slope_space)\nError = np.zeros((200,200))\n\n# fill in the error matrix\nfor row in range(np.shape(I)[0]):\n    for col in range(np.shape(S)[1]):\n        Error[row,col] = cost_function(y, I[row,col]+S[row,col]*x)\n\n\n\nThen, we can visualize our best estimate on this error space:\n\nC = plt.pcolormesh(intercept_space,slope_space, np.log10(np.sqrt(Error)))\nplt.contour(intercept_space,slope_space, np.log10(np.sqrt(Error)),colors='white',linewidths=0.7)\nplt.plot(b, m, 'wo')\nplt.text(b+2, m, '$\\\\leftarrow$ Best Fit Parameters',color='white',va='center')\nplt.colorbar(C, label='log(cost)')\nplt.xlabel('intercept ($b$)')\nplt.ylabel('slope ($m$)')\nplt.show()\n\n\n\nAs we can see, out best fit parameters are those which minimize the error. But how does this work?","type":"content","url":"/linear-regression#the-mechanics-of-linear-regression","position":7},{"hierarchy":{"lvl1":"Linear Regression as a Machine Learning Model","lvl3":"Computing the minimum of the cost function","lvl2":"Linear Regression with numpy"},"type":"lvl3","url":"/linear-regression#computing-the-minimum-of-the-cost-function","position":8},{"hierarchy":{"lvl1":"Linear Regression as a Machine Learning Model","lvl3":"Computing the minimum of the cost function","lvl2":"Linear Regression with numpy"},"content":"In the above example, we visualized the error for a relatively large range of slope and intercept values - we might refer to this as the “brute force” method. This is possible because we have a very simple model and a simple cost function.\n\nTo see how the solution is computed, first we need to consider how the problem could be formulated as a matrix set of equations. Note that in this process, we are looking for values b and m such that the difference between model values \\hat{y} and data values y are minimimized (as computed by the MSE cost function). The model values are given by the following set of equations for our N input data values:\\begin{align*}\n\\hat{y}_1 & = m x_1 + b \\\\ \n\\hat{y}_2 & = m x_2 + b \\\\ \n&\\vdots \\\\\n\\hat{y}_N & = m x_N + b \\\\ \n\\end{align*}\n\nWe can organize these in a vector format as follows:\\begin{bmatrix} \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_N \\end{bmatrix} = m \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\end{bmatrix} + b\\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\\\\\n\nUsing a boldface notation for vectors, we might also write this as\\hat{\\textbf{y}} = m \\textbf{x} + b\n\nHowever, we can also go one step further to utilize the dot product notation:\\hat{\\textbf{y}} = \\begin{bmatrix} \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_N \\end{bmatrix} = m \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\end{bmatrix} + b\\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots\\\\ 1 & x_N \\end{bmatrix} \\cdot \\begin{bmatrix} b \\\\ m \\end{bmatrix} = \\textbf{X} \\cdot \\textbf{w}\n\nwhere here we have defined a matrix X which has a column of ones corresponding to the slope value b and another column with the data values, and our model is now simply denoted as\\hat{\\textbf{y}} = \\textbf{X} \\cdot \\textbf{w}\n\nGiven, this system of equations, it can be shown (using a few derivatives) that the following value of w results in a minimum of the MSE cost function J defined above:\\textbf{w} = (X^TX)^{-1}\\textbf{X}^T\\hat{\\textbf{y}}\n\nHere, ^T denotes matrix transpose and ^{-1} indicates matrix inversion. I am not showing the derivation here, but we can try this out to ensure that the results match (they should since this is essentially what numpy is doing!):\n\n# define X\nX = np.column_stack([np.ones_like(x), x])\n\n# ensure y is a column vector\ny_vector = y.reshape((np.size(y),1))\n\n# run the computation above\nw = np.linalg.inv(X.T@X)@X.T@y_vector\n\n# print the estimated slope and intercept values\nprint('Intercept Check:',w[0,0])\nprint('Slope Check:',w[1,0])\n\n\n\nPhew, looks good - same answer as before!\n\n","type":"content","url":"/linear-regression#computing-the-minimum-of-the-cost-function","position":9},{"hierarchy":{"lvl1":"Linear Regression as a Machine Learning Model","lvl2":"Linear Regression as an Iterative Process"},"type":"lvl2","url":"/linear-regression#linear-regression-as-an-iterative-process","position":10},{"hierarchy":{"lvl1":"Linear Regression as a Machine Learning Model","lvl2":"Linear Regression as an Iterative Process"},"content":"The above problem is very simple and it is unique in that it can be solved pretty straight-forwardly with pen, paper, some calculus, and a bit of patience. This is called the “analytical solution” or the “closed form” solution. However, that’s not always the case. One alternative is to approach this as an iterative process instead of a deterministic process. In other words, if we have a guess at a set of parameters (the slope and the intercept), we can use the geometry of the error space to determine the right set of parameters by making updates to our guess?\n\nTo facilitate this process, we consider our cost function:J = \\frac{1}{N} \\sum_{i=1}^N (y_{data,i} - y_{model,i})^2 = \\frac{1}{N} \\sum_{i=1}^N (y_{data,i} - (mx_i+b))^2\n\nTo improve estimates of m and b we need to know how to “move” in our cost function space toward the minimum location. If we envision this space as a big valley, we know we need to move downhill toward the lowest point. Thus, we need to know the slope of our hill i.e. the gradient of our cost function. Luckily, that’s not so bad to compute:\\begin{align*}\n\\frac{\\partial J}{\\partial b} & = \\frac{-2}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)\\\\\n\\frac{\\partial J}{\\partial m} & = \\frac{-2}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)x_i\n\\end{align*}\n\nAfter we’ve computed the cost, we code it up in numpy:\n\ndef cost_function_gradient(x_data, y_data, y_modeled):\n    N = np.size(y_data)\n    intercept_gradient = -1*(2/N)*np.sum((y_data-y_modeled))\n    slope_gradient = -1*(2/N)*np.sum((y_data-y_modeled)*x_data)\n    gradient = np.array([intercept_gradient, slope_gradient])\n    return(gradient)\n\n\n\nTo use our cost function gradient, let’s make an initial guess and define a learning rate:\n\nintercept = -15.0                      # starting intercept guess\nslope = 4.0                            # starting slope guess\nweights = np.array([intercept, slope])\n\n\n\nWith a look to the future, we will also define a “learning rate” which will scale how big our steps are to the minimum location.\n\nlearning_rate = 0.01\n\n\n\nTo complete one “iteration”, we can update our initial guess as follows:\n\n# compute the modeled values\ny_modeled = weights[0]+weights[1]*x\n\n# compute the gradient\nweight_gradient = cost_function_gradient(x, y, y_modeled)\nprint(weight_gradient)\n\n# update the weights\nweights -= learning_rate*weight_gradient\nprint(weights)\n\n\n\nWe can imagine doing this over and over again until we converge on a best estimate. Let’s build a slider to examine how this would look over many iterations\n\ndef plot_fit_and_cost(initial_guess, n_iterations):\n\n    weights = np.copy(initial_guess)\n    for n in range(n_iterations):\n        y_modeled = weights[0]+weights[1]*x\n        weight_gradient = cost_function_gradient(x, y, y_modeled)\n        weights -= learning_rate*weight_gradient\n    \n    fig = plt.figure(figsize=(11,5))\n    \n    plt.subplot(1,2,1)\n    plt.plot(x, y,'k.')\n    plot_x = np.linspace(np.min(x), np.max(x), 100)\n    modeled_y = weights[0] + weights[1]*plot_x\n    plt.plot(plot_x, modeled_y,'b-',\n            label='Fit: '+'{:.2f}'.format(weights[1])+'*x + ' +'{:.2f}'.format(weights[0]))\n    plt.plot(plot_x, b + m*plot_x,'b--',\n             label='Best Fit: '+'{:.2f}'.format(m)+'*x + ' +'{:.2f}'.format(b))\n    plt.title('Fit after '+str(n_iterations)+' iteration(s)')\n    plt.legend(loc=2)\n    plt.xlim([-1,11])\n    plt.ylim([-5,30])\n    plt.ylabel('y')\n    plt.xlabel('x')\n    \n    plt.subplot(1,2,2)\n    C = plt.pcolormesh(intercept_space,slope_space, np.log10(Error))\n    plt.contour(intercept_space,slope_space, np.log10(Error),colors='white',linewidths=0.7)\n    plt.plot(initial_guess[0], initial_guess[1], 'wo')\n    plt.plot(weights[0], weights[1], 'wo')\n    plt.text(initial_guess[0]+2, initial_guess[1], '$\\\\leftarrow$ Initial',color='white',va='center')\n    if n_iterations>0:\n        plt.text(weights[0]+2, weights[1], '$\\\\leftarrow$ Final',color='white',va='center')\n    plt.colorbar(C, label='log(cost)')\n    plt.title('Error space')\n    plt.ylabel('slope ($m$)')\n    plt.xlabel('intercept ($b$)')\n    plt.show()\n\n\n\n\n# this code block is commented for publication of the jupyter book\n# uncomment this line when running on your local machine\n# interact(plot_fit_and_cost, initial_guess=fixed(np.array([intercept, slope])),\n#         n_iterations=widgets.IntSlider(min=0, max=1000));\n\n\n\nBy sliding the iteration counter, we see that each update brings us close to the minimum in the cost function. In the above demonstration, we say that we have “learned” the right parameters for our model by “training” it on our data.\n\nNote\n\nThe above slide will not work on Jupyter books. To use this demo, download the notebook and the data (or simply clone this repository) and run the demo on your machine.\n\nKey Takeaways\n\nA linear model can be expressed in matrix form as\\hat{\\textbf{y}} = \\textbf{X} \\cdot \\textbf{w}\n\nThe optimum parameters for the model (w) are those which minimize a cost function - the metric for how the model predictions differ from the data.\n\nBy considering the gradient of our cost function relative to the model parameters, we can solve for the parameters iteratively by slowly taking steps toward a minimum in the cost function. This process is called gradient descent.","type":"content","url":"/linear-regression#linear-regression-as-an-iterative-process","position":11},{"hierarchy":{"lvl1":"Implementing Linear Regression as a Machine Learning Model"},"type":"lvl1","url":"/linear-regression-class","position":0},{"hierarchy":{"lvl1":"Implementing Linear Regression as a Machine Learning Model"},"content":"In this notebook, we will revisit linear regression and consider implementing it as a trainable machine learning model.\n\nLearning Objectives\n\nBy the end of this notebook, you should be able to\n\nList some machine learning key terms and identify how they differ from notation in traditional math textbooks.\n\nWrite a Python class to implement linear regression.\n\nVisualize the loss function of a model as a function of the training iterations.\n\nImport modules\n\nBegin by importing the modules to be used in this notebook\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\n\nIn the previous notebook, we saw how we could fit a linear model to some data using an iterative approach. In this notebook, we are going to continue with this theme, abstracting some of the key ideas into a structure and approach we can use for a wide variety of problems.","type":"content","url":"/linear-regression-class","position":1},{"hierarchy":{"lvl1":"Implementing Linear Regression as a Machine Learning Model","lvl2":"Formulating a Model Framework"},"type":"lvl2","url":"/linear-regression-class#formulating-a-model-framework","position":2},{"hierarchy":{"lvl1":"Implementing Linear Regression as a Machine Learning Model","lvl2":"Formulating a Model Framework"},"content":"To begin, let’s reconsider our linear regression model and introduce some new terminology.\n\nIn our math classes, we describe the line we are trying to fit to the data as having a slope w (err, perhaps m) and intercept b. We combine the these constants together in line with the equation\\hat{y} = wx + b\n\nwhere the “hat” symbol above the y indicates it is a model estimate of some true value y. In the world of Machine Learning, we will adopt some new terminology.","type":"content","url":"/linear-regression-class#formulating-a-model-framework","position":3},{"hierarchy":{"lvl1":"Implementing Linear Regression as a Machine Learning Model","lvl3":"The Forward Model","lvl2":"Formulating a Model Framework"},"type":"lvl3","url":"/linear-regression-class#the-forward-model","position":4},{"hierarchy":{"lvl1":"Implementing Linear Regression as a Machine Learning Model","lvl3":"The Forward Model","lvl2":"Formulating a Model Framework"},"content":"The first update to our terminology is that we will start by calling w a weight and b a bias. These parameters are utilized by our model to produce a predicted value. This part of the line-fitting process is what we’ll call the Forward Model.\n\nIn our previous notebook, we saw that would could write our equations in matrix form as\\begin{bmatrix} \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_N \\end{bmatrix} = w \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\end{bmatrix} + b\\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\\\\\n\nor, more succinctly as\\hat{\\textbf{y}} = \\textbf{X} \\cdot \\textbf{w}\n\n","type":"content","url":"/linear-regression-class#the-forward-model","position":5},{"hierarchy":{"lvl1":"Implementing Linear Regression as a Machine Learning Model","lvl3":"Training the Model","lvl2":"Formulating a Model Framework"},"type":"lvl3","url":"/linear-regression-class#training-the-model","position":6},{"hierarchy":{"lvl1":"Implementing Linear Regression as a Machine Learning Model","lvl3":"Training the Model","lvl2":"Formulating a Model Framework"},"content":"Once we have our predicted values, we can compare with our target values to determine our error. We compute these differences using a Loss Function (which in math class and our previous notebook we have referred to as a “cost function” or simply an error). For linear regression, our Loss Function is the Mean Square ErrorL =\\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2\n\nBased on these differences, we can then update our weight and bias to produce predicted values which are closer to our target values. This step is the Model Training step and utilizes the idea of gradient descent. In our previous notebook, we computed the gradients of our cost function relative to both the weight and the bias:\\begin{align*}\n\\frac{\\partial L}{\\partial b} & = \\frac{-2}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i))\\\\\n\\frac{\\partial L}{\\partial w} & = \\frac{-2}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)x_i\n\\end{align*}\n\nWe can simplify this in notation considerably again using vector notation. Let’s collect the two partial derivatives into a vector derivative with\\begin{align*}\n\\frac{\\partial L}{\\partial \\textbf{w}} &= \\begin{bmatrix} \\frac{\\partial L}{\\partial b} & \\frac{\\partial L}{\\partial w} \\end{bmatrix} \\\\\n&= \\frac{-2}{N}  \\begin{bmatrix} \\sum_{i=1}^N (y_i - \\hat{y}_i)) & \\sum_{i=1}^N (y_i - \\hat{y}_i)x_i \\end{bmatrix} \\\\\n&= \\frac{-2}{N} \\begin{bmatrix} (y_1 - \\hat{y}_1) (y_2 - \\hat{y}_2) \\cdots (y_N - \\hat{y}_N) \\end{bmatrix} \\cdot \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots\\\\ 1 & x_N \\end{bmatrix}\\\\\n&= \\frac{-2}{N} (\\textbf{y} - \\hat{\\textbf{y}})^T \\cdot \\textbf{X}\n\\end{align*}\n\nOr  more concisely:\\frac{\\partial L}{\\partial \\textbf{w}} = \\frac{-2}{N} (\\textbf{y} - \\hat{\\textbf{y}})^T \\cdot \\textbf{X}\n\nThis notation will be particularly advantageous when we code up our solution below.\n\nHere we have taken our simple example and given it a bit of a structure that will allow us to apply it to a large range of applications. This process - the construction of a forward model and the definition of the loss function - is at the heart of many approaches in Machine Learning algorithms.\n\nWe can put these steps together into a diagram as follows:\n\n\n\n","type":"content","url":"/linear-regression-class#training-the-model","position":7},{"hierarchy":{"lvl1":"Implementing Linear Regression as a Machine Learning Model","lvl2":"Defining a Linear Regression Python Class"},"type":"lvl2","url":"/linear-regression-class#defining-a-linear-regression-python-class","position":8},{"hierarchy":{"lvl1":"Implementing Linear Regression as a Machine Learning Model","lvl2":"Defining a Linear Regression Python Class"},"content":"Now that we’ve got some notation out of the way, let’s define a Python structure that utilizes our generalized notation. We will do this in the form of a class which we use to make objects. Many of the existing Machine Learning packages use this approach, so we will follow with this paradigm. Let’s check out out example:\n\nclass LinearRegression:\n    \"\"\"\n    A python class implementation of a linear regression model.\n    \"\"\"\n\n    def __init__(self, X, learning_rate, n_iterations, random_seed=1):\n        \"\"\"\n        Initializes the LinearRegression model and its parameters.\n        \"\"\"\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.random_seed = random_seed\n        self.initialize(X)\n\n    def initialize(self, X):\n        \"\"\"\n        Initializes the weight vector with small random values.\n        \"\"\"\n        np.random.seed(self.random_seed)\n        self.w = np.random.normal(loc=0.0, scale=0.01, size=np.shape(X)[1])\n    \n    def forward(self, X):\n        \"\"\"\n        Computes the predicted output using the current weights.\n        \"\"\"\n        y_hat = np.dot(X, self.w)\n        return y_hat\n\n    def loss(self, y, y_hat):\n        \"\"\"\n        Computes the mean sum of squared errors loss.\n        \"\"\"\n        return (1/np.size(y))*np.sum((y - y_hat) ** 2)\n\n    def loss_gradient(self, X, y, y_hat):\n        \"\"\"\n        Computes the gradient of the loss with respect to weights.\n        \"\"\"\n        N = np.size(y)\n        gradient = (-2 / N) * np.dot((y-y_hat).T,X)\n        return gradient\n\n    def fit(self, X, y):\n        \"\"\"\n        Trains the linear regression model using gradient descent.\n        \"\"\"\n        self.losses = []\n        for iteration in range(self.n_iterations):\n            y_hat = self.forward(X)\n            gradient = self.loss_gradient(X, y, y_hat)\n            self.w -= self.learning_rate * gradient\n            self.losses.append(self.loss(y, y_hat))\n\n\n\n","type":"content","url":"/linear-regression-class#defining-a-linear-regression-python-class","position":9},{"hierarchy":{"lvl1":"Implementing Linear Regression as a Machine Learning Model","lvl3":"Using a Model Object","lvl2":"Defining a Linear Regression Python Class"},"type":"lvl3","url":"/linear-regression-class#using-a-model-object","position":10},{"hierarchy":{"lvl1":"Implementing Linear Regression as a Machine Learning Model","lvl3":"Using a Model Object","lvl2":"Defining a Linear Regression Python Class"},"content":"Just as in our first notebook, let’s take a look at some data. Here, we’ll use the same dataset for carbon dioxide concentration as we did previously:\n\n# read in the dataset as shown in the previous notebook\ndf = pd.read_csv('../data/monthly_in_situ_co2_mlo.csv', skiprows=64)\ndf = df[df.iloc[:, 4] >0]\n\n# subset to a given time range\ndf_subset = df[(df.iloc[:,3]>=2010) & (df.iloc[:,3]<=2020)]\n\n# redefine x and y\nx = np.array(df_subset.iloc[:,3])\ny = np.array(df_subset.iloc[:,4])\n\n# remove the first value of y\nx = x-2010\ny = y-y[0]\n\n\n\nAs a reminder, these points were historically collected using individual measurements! The following picture shows Charles Keeling taking one such measurement:\n\nPhoto Credit: Keeling Family, \n\nScripps Institute of Oceanography\n\nUsing the notation defined above, let’s make our X matrix using our data:\n\nX = np.column_stack([np.ones((np.size(x),)), x])\n\n\n\nNext, we can drop this into our model:\n\nmodel = LinearRegression(X, learning_rate=0.01, n_iterations=50)\n\n\n\nNow, using the forward method of our class, we can test out how our model is doing:\n\ny_predict = model.forward(X)\n\n\n\nplt.figure(figsize=(8,4))\nplt.plot(x,y,'k.',label='$y$ (data)')\nplt.plot(x,y_predict,'b-',label='$\\\\hat{y}$ (model)')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend(loc=2)\nplt.show()\n\n\n\nAs we can see, our model isn’t good at all - and that’s because we haven’t “trained” it on our data yet. Let’s check the weight and the bias of our model:\n\nprint('Bias:',model.w[0])\nprint('Weight:',model.w[1])\n\n\n\nThese values were simply generated with the random value generator in the initialize function. We use random values for initializing the bias and weight rather than 0 so that the gradients can be computed.\n\nLet’s go ahead and train our model.\n\n","type":"content","url":"/linear-regression-class#using-a-model-object","position":11},{"hierarchy":{"lvl1":"Implementing Linear Regression as a Machine Learning Model","lvl3":"Training a Model Object","lvl2":"Defining a Linear Regression Python Class"},"type":"lvl3","url":"/linear-regression-class#training-a-model-object","position":12},{"hierarchy":{"lvl1":"Implementing Linear Regression as a Machine Learning Model","lvl3":"Training a Model Object","lvl2":"Defining a Linear Regression Python Class"},"content":"The model can be trained with the fit function implemented inthe class.\n\nmodel.fit(X,y)\n\n\n\nAfter calling the fit function, we see that the bias and the weight have changes:\n\nprint('Bias:',model.w[0])\nprint('Weight:',model.w[1])\n\n\n\nLet’s see how our model looks now:\n\ny_predict = model.forward(X)\n\n\n\nplt.figure(figsize=(8,4))\nplt.plot(x,y,'k.',label='$y$ (data)')\nplt.plot(x,y_predict,'b-',label='$\\\\hat{y}$ (model)')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend(loc=2)\nplt.show()\n\n\n\nClearly, this is a much better fit than before!\n\nOne important thing to consider in the training of our model is how well the model is fitting our data. This is given by the loss function. In the class above, the losses we tallied on each model iteration so we can examine those on a curve:\n\nplt.figure(figsize=(8,4))\nplt.semilogy(np.arange(50), model.losses)\nplt.xlabel('Iterations')\nplt.ylabel('Loss')\nplt.show()\n\n\n\nAs we can see, the losses took a deep plunge in the first 5 iterations and started to plateau after that. A plateau in the loss function is a good indication that the model solution is converging to the best fit parameters for the data. We will be investigating lots of loss functions in this book because it is one of the key metrics for assessing a model.\n\nKey Takeaways\n\nThe loss function gradients with respect to model parameters can be expressed in vector notation as\\frac{\\partial L}{\\partial \\textbf{w}} = \\frac{-2}{N} (\\textbf{y} - \\hat{\\textbf{y}})^T \\cdot \\textbf{X}\n\nThe iterative solution of model parameters can be built into a Python class with a series of functions that initialize and fit the model to data.\n\nThe model loss function provides key information on the convergence of the model parameters to the “best guess”.","type":"content","url":"/linear-regression-class#training-a-model-object","position":13},{"hierarchy":{"lvl1":"Overview"},"type":"lvl1","url":"/overview","position":0},{"hierarchy":{"lvl1":"Overview"},"content":"Motivation\n\nMachine learning is a popular topic these days, but in reality, it has been a topic of study for quite some time as a subset of data science. As a transition to thinking about machine learning approaches and algorithms, it’s helpful to consider a simple example common to data analysis - linear regression. In the subsequent subsections under Machine Learning Basics, we’ll use a set of linear regression examples to introduce machine learning terminology and a common framework that will apply to other algorithms and applications down the road. In this notebook, we’ll take a look at our example data - CO_2 concentrations collected at the Mauna Loa Observatory a.k.a. the “Keeling Curve”.\n\nImport modules\n\nBegin by importing the modules to be used in this notebook:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\n\n","type":"content","url":"/overview","position":1},{"hierarchy":{"lvl1":"Overview","lvl2":"Mauna Loa CO_2 Measurements"},"type":"lvl2","url":"/overview#mauna-loa-co-2-measurements","position":2},{"hierarchy":{"lvl1":"Overview","lvl2":"Mauna Loa CO_2 Measurements"},"content":"For a concrete example, we will consider the trend in CO_2 concentration in the atmosphere as measured at the Mauna Loa Observatory on the Mauna Loa volcano in Hilo Hawaii:\n\n\n\nPhoto Credit: Johnathon Kingston, \n\nNational Geographic.\n\nThe record of CO_2 concentrations collected at Mauna Loa, sometimes referred to as the “Keeling Curve” for its founding scientist, is a multi-decadal record that is available from the Scripps Institute of Oceanography \n\nHERE. The video \n\nHERE for information on how the data is collected.\n\nHere, we will work the monthly data in this dataset, which is available in the monthly_in_situ_co2_mlo.csv file provided in the data directory of this book. Let’s read that in here:\n\n# read in the dataset with pandas\ndf = pd.read_csv('../data/monthly_in_situ_co2_mlo.csv', skiprows=64)\n\n# filter out null values stored as -99\ndf = df[df.iloc[:, 4] >0]\n\n# the decimal year information is in the 4th column\nx = df.iloc[:, 3]\n\n# the CO2 information is in the 5th column\ny = df.iloc[:, 4]\n\n\n\nLet’s take a look at what this data looks like:\n\nplt.figure(figsize=(8,4))\nplt.plot(x,y,'k.')\nplt.xlabel('x (year)')\nplt.ylabel('y (CO$_2$ concentration, ppm)')\nplt.show()\n\n\n\nAs we can see, this record extends back to the 1950’s and continues through present day. We’ll take a number of approaches to investigate this data in the next notebooks.","type":"content","url":"/overview#mauna-loa-co-2-measurements","position":3},{"hierarchy":{"lvl1":"Installing Python and Jupyter"},"type":"lvl1","url":"/setting-up-python-and-jupyter","position":0},{"hierarchy":{"lvl1":"Installing Python and Jupyter"},"content":"","type":"content","url":"/setting-up-python-and-jupyter","position":1},{"hierarchy":{"lvl1":"Installing Python and Jupyter","lvl2":"Download Miniconda"},"type":"lvl2","url":"/setting-up-python-and-jupyter#download-miniconda","position":2},{"hierarchy":{"lvl1":"Installing Python and Jupyter","lvl2":"Download Miniconda"},"content":"There are many ways to install python for use on your machine. One of the most popular ways is to use Anaconda - a distribution of Python designed for data science. Anaconda contains many programs that may not be applicable to your application. Miniconda, on the other hand, is a paired-down version of Anaconda that allows you to install new modules as you need them. As a result, it takes up far less storage on your machine. Miniconda will include Python (the programming language) and conda (the package manager).\n\nTo download Miniconda, navigate to \n\nhttps://​docs​.conda​.io​/en​/latest​/miniconda​.html and choose a Conda Installer that pertains to your system. For Windows machines, the majority of machines are 64-bit. For MacOS, choose the pkg file which pertains to your chip (see \n\nHERE).","type":"content","url":"/setting-up-python-and-jupyter#download-miniconda","position":3},{"hierarchy":{"lvl1":"Installing Python and Jupyter","lvl2":"Set up a conda environment"},"type":"lvl2","url":"/setting-up-python-and-jupyter#set-up-a-conda-environment","position":4},{"hierarchy":{"lvl1":"Installing Python and Jupyter","lvl2":"Set up a conda environment"},"content":"After you have downloaded miniconda, set up a conda environment. Begin by opening up a terminal of your choice. On Windows, you can open the “Anaconda Prompt” application by searching for it in the start menu. On Mac, you can open a standard Terminal in the Applications/Utilities directory.\n\nWith your terminal open, create a conda environment called ms285 with Python version 3.12 using the following command:conda create --name ms285 python=3.12\n\nBy default, conda will automatically activate a “base” environment on your machine. To disable this behavior, you may want to run the following:conda config --set auto_activate_base false\n\nNow, activate your environment as follows:conda activate ms285\n\nEvery time you use your environment, you will need to activate it from the command line.\n\nNext, download the pertinent modules required for this course:conda install numpy\nconda install matplotlib\nconda install pytorch\nconda install pandas\nconda install netcdf4\nconda install scipy\nconda install scikit-learn\nconda install jupyter\nconda install jupyterlab\nconda install git\n\nNote\n\nIf any of the conda installations don’t work, you can specify the source directly. For example, scikit-learn is available via conda-forge asconda install conda-forge::scikit-learn\n\nFinally, configure your environment to use in a jupyter notebook.\n\nOn MacOS, use:python3 -m ipykernel install --user --name=ms285\n\nOn Windows, use:python -m ipykernel install --user --name=ms285","type":"content","url":"/setting-up-python-and-jupyter#set-up-a-conda-environment","position":5},{"hierarchy":{"lvl1":"Introduction to Machine Learning"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Introduction to Machine Learning"},"content":"Welcome to an Introduction to Machine Learning!","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl2":"About This Book"},"type":"lvl2","url":"/#about-this-book","position":2},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl2":"About This Book"},"content":"The materials in this “book” originated from course notes in the CS 171 Introduction to Machine Learning course at San José State University and I am slowly posting and updating them for a graduate-level MS 285 Machine Learning Application in Marine Science seminar course at the Moss Landing Marine Labs. Most of the examples are related to marine science, oceanography, climate science, or related fields.\n\nThese notes are intended to be an introduction to core topics in machine learning and a guide for students and researchers in both computer science and marine science. PyTorch is the primary computational tool used to illustrate models and workflows because it is flexibile and used widely in research. However, there are many examples that used regular computational tools like numpy and scipy and a few selected examples also use scikit-learn.\n\nNote\n\nThis book is a work in progress and will continue to be updated through Spring 2025 and beyond. If you have any feedback or constructive suggestions, feel free to submit an issue on the \n\nGithub repository for this book. I appreciate your feedback!","type":"content","url":"/#about-this-book","position":3},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl2":"Table of Contents"},"type":"lvl2","url":"/#table-of-contents","position":4},{"hierarchy":{"lvl1":"Introduction to Machine Learning","lvl2":"Table of Contents"},"content":"Introduction to Machine Learning\n\nGetting Started\n\nInstalling Python and Jupyter\n\nBasics\n\nOverview\n\nLinear Regression as a Machine Learning Model\n\nImplementing Linear Regression as a Machine Learning Model\n\nGetting Started\n\nReferences","type":"content","url":"/#table-of-contents","position":5},{"hierarchy":{"lvl1":"References"},"type":"lvl1","url":"/references","position":0},{"hierarchy":{"lvl1":"References"},"content":"","type":"content","url":"/references","position":1}]}